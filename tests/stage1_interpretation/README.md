# Stage 1 Tests: Natural Language -> LTLf Generation

This directory contains tests for Stage 1 of the pipeline, which converts natural language instructions into LTLf (Linear Temporal Logic on Finite Traces) formulas.

## Files

- **`test_cases_nl_to_ltlf.csv`**: Test cases with expected LTLf outputs
- **`test_nl_to_ltlf_generation.py`**: Test runner script
- **`test_results_nl_to_ltlf_*.csv`**: Generated test results (timestamped)

## Test Case Format

Each test case in `test_cases_nl_to_ltlf.csv` contains:

| Column | Description |
|--------|-------------|
| `test_id` | Unique identifier (e.g., 1, 2, 3) |
| `category` | Test category (e.g., simple_goal, nested_operators) |
| `natural_language` | Natural language instruction |
| `expected_ltlf` | Expected LTLf formula(s), comma-separated |
| `expected_objects` | Expected object list (Python list format) |
| `description` | Brief description of the test |
| `notes` | Additional notes about what's being tested |

## Running Tests

### Run All Tests
```bash
cd tests/stage1_interpretation
python test_nl_to_ltlf_generation.py
```

### Output
- Console output shows real-time progress and results
- CSV file generated with timestamp: `test_results_nl_to_ltlf_YYYYMMDD_HHMMSS.csv`

## Test Results Format

The output CSV includes:

| Column | Description |
|--------|-------------|
| All input columns | From test_cases_nl_to_ltlf.csv |
| `actual_ltlf` | LTLf formula(s) generated by LLM |
| `actual_objects` | Objects extracted by LLM |
| `success` | True/False - exact match |
| `match_type` | "exact", "partial", "wrong", "error" |
| `error_message` | Error details if applicable |
| `reflection` | Analysis of failure and improvement suggestions |

## Test Categories

### 1. Simple Goals (`simple_goal`)
- Basic F (Eventually) operator
- Single predicate goals
- Alternative phrasings

### 2. Multiple Goals (`multiple_goals`, `conjunctive_goal`)
- Multiple independent F operators
- Different predicates in same goal

### 3. Temporal Constraints (`globally_constraint`, `mixed_temporal`)
- G (Globally/Always) operator
- Combination of F and G
- Safety properties

### 4. Advanced Operators (`next_operator`, `until_operator`)
- X (Next) for sequential actions
- U (Until) for conditional maintenance

### 5. Nested Operators (`nested_fg`, `nested_gf`)
- F(G(φ)) - "eventually always"
- G(F(φ)) - "always eventually"

### 6. Complex Scenarios (`tower_building`, `complex_tower`)
- Multi-level structures
- Multiple related predicates

### 7. Edge Cases (`negative_constraint`, `ambiguous_order`)
- Negation
- Temporal ordering
- Conjunction placement

## Interpreting Results

### Match Types

- **exact**: Perfect match between expected and actual
- **partial**: Some formulas correct, others missing
- **wrong**: Completely different output
- **error**: Exception occurred during parsing

### Reflection Messages

The `reflection` column provides:
- Root cause analysis
- Specific operator issues (missing F, G, X, U)
- Object extraction problems
- Prompt improvement suggestions

### Example Reflections

```
Missing F (Eventually) operator - prompt may not emphasize temporal goals
Object mismatch - Expected: ['a', 'b'], Got: ['a', 'b', 'c'] | Improvement: Enhance object extraction in prompt
Partial match - some formulas correct, others missing or wrong | Improvement: Clarify conjunctive vs separate goals in prompt
```

## Adding New Test Cases

1. Add row to `test_cases_nl_to_ltlf.csv`
2. Use next available `test_id`
3. Choose appropriate `category` or create new one
4. Write clear `natural_language` instruction
5. Specify `expected_ltlf` (comma-separated if multiple)
6. Specify `expected_objects` as Python list string
7. Add helpful `description` and `notes`

## Analyzing Results

### Quick Analysis
```bash
# Count pass/fail
grep -c ",True," test_results_nl_to_ltlf_*.csv
grep -c ",False," test_results_nl_to_ltlf_*.csv

# Show only failures
grep ",False," test_results_nl_to_ltlf_*.csv | cut -d',' -f1-3
```

### Python Analysis
```python
import pandas as pd

# Load results
df = pd.read_csv('test_results_nl_to_ltlf_TIMESTAMP.csv')

# Success rate by category
print(df.groupby('category')['success'].mean())

# Common failure patterns
failures = df[df['success'] == False]
print(failures[['test_id', 'match_type', 'reflection']])
```

## Continuous Improvement

1. **Identify Patterns**: Look for categories with low success rates
2. **Update Prompts**: Modify `src/stage1_interpretation/prompts.py` based on reflections
3. **Re-run Tests**: Verify improvements
4. **Add Edge Cases**: Add new test cases for discovered issues
5. **Iterate**: Repeat cycle

## Notes

- Tests use real LLM API calls (costs money)
- Each test case takes ~1-3 seconds
- Full test suite may take several minutes
- Results are deterministic with temperature=0.0
- Consider running subsets during development
