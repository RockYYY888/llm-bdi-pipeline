# Stage 3 Optimization Opportunities

## å‘ç°çš„é‡å¤å’Œå†—ä½™

### ğŸ”´ Critical: Ground Actionsåœ¨æ¯ä¸ªstateé‡å¤è®¡ç®—

**é—®é¢˜ä½ç½®**: `forward_planner.py:167-177`

```python
while queue:  # For each state in BFS queue
    current_state = queue.popleft()

    for grounded_action in self._ground_all_actions():  # â† æ¯æ¬¡éƒ½é‡æ–°ground!
        # Check preconditions...
```

**å…·ä½“é—®é¢˜**:
- `_ground_all_actions()` åœ¨**æ¯ä¸ªstate**éƒ½è¢«è°ƒç”¨ä¸€æ¬¡
- å¯¹äº2 blocks: æ¢ç´¢1093ä¸ªstates â†’ è°ƒç”¨1093æ¬¡
- æ¯æ¬¡éƒ½é‡æ–°è®¡ç®— `itertools.product(objects, repeat=n)`
- å®é™…ä¸Šground actionså¯¹äºå›ºå®šçš„domainå’Œobjectsæ˜¯**å®Œå…¨ä¸å˜çš„**ï¼

**æ€§èƒ½å½±å“**:
```
2 blocks, 1093 states:
- Current: 1093 Ã— 32 ground actions = 34,976æ¬¡groundingè®¡ç®—
- Optimal: 1æ¬¡ Ã— 32 ground actions = 32æ¬¡groundingè®¡ç®—
- æµªè´¹: 1093x = 99.9%çš„é‡å¤è®¡ç®—
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
class ForwardStatePlanner:
    def __init__(self, domain, objects):
        self.domain = domain
        self.objects = objects
        self.grounded_actions = self._ground_all_actions()  # â† åªè®¡ç®—ä¸€æ¬¡

    def explore_from_goal(...):
        while queue:
            current_state = queue.popleft()

            for grounded_action in self.grounded_actions:  # â† ç›´æ¥ä½¿ç”¨ç¼“å­˜
                # Check preconditions...
```

**é¢„æœŸæå‡**:
- 2 blocks: å‡å°‘99.9%çš„groundingè®¡ç®—
- 3 blocks: å‡å°‘99.98%çš„groundingè®¡ç®—ï¼ˆå‡è®¾50k statesï¼‰
- æ—¶é—´èŠ‚çœä¼°è®¡: **20-30%æ€»ä½“é€Ÿåº¦æå‡**

---

### ğŸŸ  Important: ç›¸åŒGoalçš„é‡å¤æ¢ç´¢

**é—®é¢˜ä½ç½®**: `backward_planner_generator.py:105-145`

```python
all_code_sections = []

for transition in dfa_transitions:
    goal = parse_label(transition.label)  # "on_a_b"

    # æ¯ä¸ªtransitionéƒ½ç‹¬ç«‹æ¢ç´¢ï¼Œå³ä½¿goalç›¸åŒ
    state_graph = planner.explore_from_goal(goal)  # â† é‡å¤æ¢ç´¢
    code = codegen.generate(state_graph)
    all_code_sections.append(code)
```

**å…·ä½“é—®é¢˜**:
å¦‚æœDFAä¸­æœ‰å¤šä¸ªtransitionsä½¿ç”¨ç›¸åŒçš„labelï¼Œä¼šé‡å¤æ¢ç´¢ï¼š

```
DFA example:
state0 --[on_a_b]--> state1
state2 --[on_a_b]--> state3  â† ç›¸åŒçš„label!
```

å½“å‰å®ç°ä¼šå¯¹ `on_a_b` æ¢ç´¢**ä¸¤æ¬¡**ï¼Œç”Ÿæˆ**ä¸¤æ¬¡**å®Œå…¨ç›¸åŒçš„state graphå’Œcodeã€‚

**æ€§èƒ½å½±å“**:
```
å¦‚æœDFAæœ‰Nä¸ªtransitionsä½¿ç”¨ç›¸åŒgoal:
- Current: Næ¬¡å®Œæ•´æ¢ç´¢ï¼ˆN Ã— 1093 statesï¼‰
- Optimal: 1æ¬¡æ¢ç´¢ + (N-1)æ¬¡ä»£ç å¤ç”¨
- æµªè´¹: (N-1) Ã— 100%çš„é‡å¤æ¢ç´¢
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
def generate(self, ltl_dict, dfa_result):
    # Cache for goal â†’ state_graph mapping
    goal_cache = {}

    all_code_sections = []

    for transition in dfa_transitions:
        goal = parse_label(transition.label)
        goal_key = self._serialize_goal(goal)  # Convert to hashable key

        if goal_key in goal_cache:
            # Reuse cached state graph
            state_graph = goal_cache[goal_key]
            print(f"  âœ“ Reusing cached exploration for {goal_key}")
        else:
            # First time seeing this goal
            state_graph = planner.explore_from_goal(goal)
            goal_cache[goal_key] = state_graph

        code = codegen.generate(state_graph)
        all_code_sections.append(code)
```

**é¢„æœŸæå‡**:
- DFA with duplicate goals: å‡å°‘50-90%çš„æ¢ç´¢æ—¶é—´ï¼ˆå–å†³äºé‡å¤åº¦ï¼‰

---

### ğŸŸ¡ Moderate: Initial Beliefså’ŒAction Plansçš„é‡å¤ç”Ÿæˆ

**é—®é¢˜ä½ç½®**: `agentspeak_codegen.py:77-86`

```python
def generate(self):
    sections = []

    initial_beliefs = self._generate_initial_beliefs()  # â† æ¯ä¸ªtransitionéƒ½ç”Ÿæˆ
    action_plans = self._generate_action_plans()       # â† æ¯ä¸ªtransitionéƒ½ç”Ÿæˆ
    goal_plans = self._generate_goal_plans()           # Only this differs!

    sections.append(initial_beliefs)
    sections.append(action_plans)
    sections.append(goal_plans)

    return "\n\n".join(sections)
```

**å…·ä½“é—®é¢˜**:
æ¯ä¸ªtransition (code section) éƒ½ä¼šç”Ÿæˆï¼š
1. **Initial Beliefs**: `ontable(a). clear(a). ...` ï¼ˆå®Œå…¨ç›¸åŒï¼‰
2. **Action Plans**: `+!pick_up(B) : ... <- ...` ï¼ˆå®Œå…¨ç›¸åŒï¼‰
3. Goal Plans: `+!on(a, b) : ... <- ...` ï¼ˆ**å”¯ä¸€**ä¸åŒçš„éƒ¨åˆ†ï¼‰

å¯¹äº2ä¸ªtransitionsï¼š
- Initial beliefsç”Ÿæˆ**2æ¬¡**ï¼ˆæµªè´¹1æ¬¡ï¼‰
- Action plansç”Ÿæˆ**2æ¬¡**ï¼ˆæµªè´¹1æ¬¡ï¼‰

**æœ€ç»ˆAgentSpeakæ–‡ä»¶**:
```agentspeak
/* ========== Goal: on(a, b) ========== */
/* Initial Beliefs */        â† é‡å¤1
ontable(a). clear(a). ...

/* Action Plans */            â† é‡å¤2
+!pick_up(B) : ... <- ...
+!put_on_block(B1, B2) : ... <- ...

/* Goal Plans */
+!on(a, b) : ... <- ...

/* ========== Next Goal ========== */

/* ========== Goal: clear(a) ========== */
/* Initial Beliefs */        â† é‡å¤1ï¼ˆç›¸åŒï¼ï¼‰
ontable(a). clear(a). ...

/* Action Plans */            â† é‡å¤2ï¼ˆç›¸åŒï¼ï¼‰
+!pick_up(B) : ... <- ...
+!put_on_block(B1, B2) : ... <- ...

/* Goal Plans */
+!clear(a) : ... <- ...
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

**æ–¹æ¡ˆA**: åªç”Ÿæˆä¸€æ¬¡å…±äº«éƒ¨åˆ†
```agentspeak
/* Main Header */

/* ========== Shared Components ========== */

/* Initial Beliefs */
ontable(a). clear(a). handempty.

/* Action Plans */
+!pick_up(B1, B2) : ... <- ...
+!put_on_block(B1, B2) : ... <- ...
... (all action plans)

/* ========== Goal-Specific Plans ========== */

/* Goal: on(a, b) */
+!on(a, b) : ... <- ...

/* Goal: clear(a) */
+!clear(a) : ... <- ...
```

**æ–¹æ¡ˆB**: ä¿®æ”¹code generationç»“æ„
```python
class BackwardPlannerGenerator:
    def generate(self, ltl_dict, dfa_result):
        # Generate shared parts ONCE
        shared_initial_beliefs = self._generate_shared_initial_beliefs()
        shared_action_plans = self._generate_shared_action_plans()

        # Generate goal-specific parts for each transition
        all_goal_plans = []
        for transition in dfa_transitions:
            goal_plans = self._generate_goal_plans_only(transition)
            all_goal_plans.append(goal_plans)

        # Combine
        final_code = header + shared_initial_beliefs + shared_action_plans + \
                    "\n\n".join(all_goal_plans)
        return final_code
```

**æ€§èƒ½å½±å“**:
- Code generationæ—¶é—´: å‡å°‘30-50%
- ç”Ÿæˆçš„ä»£ç å¤§å°: å‡å°‘20-40%ï¼ˆå¯¹äºå¤šä¸ªtransitionsï¼‰
- **æ›´é‡è¦**: ä»£ç æ›´æ¸…æ™°ï¼Œé¿å…é‡å¤å®šä¹‰

---

### ğŸŸ¢ Nice-to-have: å¯¹ç§°æ€§ä¼˜åŒ–ï¼ˆSymmetry Reductionï¼‰

**é—®é¢˜æè¿°**:
åœ¨blocksworldä¸­ï¼Œå¾ˆå¤šstatesæ˜¯**å¯¹ç§°çš„**ï¼š

```
on(a, b) å’Œ on(b, a) çš„state spaceç»“æ„æ˜¯å¯¹ç§°çš„
åªéœ€è¦æŠŠaå’Œbäº¤æ¢å³å¯
```

**ä¾‹å­**:
```
Goal 1: on(a, b)
- æ¢ç´¢1093ä¸ªstates
- ç”Ÿæˆ26ä¸ªplans

Goal 2: on(b, a)  â† å‚æ•°åªæ˜¯äº¤æ¢äº†
- åˆæ¢ç´¢1093ä¸ªstates
- ç”Ÿæˆ26ä¸ªplansï¼ˆç»“æ„ç›¸åŒï¼Œåªæ˜¯å‚æ•°ä¸åŒï¼‰
```

**ä¼˜åŒ–æ–¹æ¡ˆ** (å¤æ‚åº¦é«˜):
1. æ£€æµ‹goalä¹‹é—´çš„å¯¹ç§°æ€§
2. åªæ¢ç´¢canonical form (å¦‚ on(x, y) where x < y)
3. é€šè¿‡å‚æ•°é‡å‘½åç”Ÿæˆsymmetric goalsçš„ä»£ç 

**é¢„æœŸæå‡**:
- ç†è®ºä¸Šå¯ä»¥å‡å°‘50%çš„æ¢ç´¢ï¼ˆå¯¹äºå¯¹ç§°domainï¼‰
- **ä½†å®ç°å¤æ‚åº¦æé«˜**ï¼Œå¯èƒ½ä¸å€¼å¾—

**å»ºè®®**: æš‚æ—¶ä¸å®ç°ï¼Œé™¤éæœ‰æ˜ç¡®çš„å¯¹ç§°æ€§éœ€æ±‚

---

## ä¼˜åŒ–ä¼˜å…ˆçº§å’Œå®æ–½å»ºè®®

### Priority 1: ğŸ”´ Critical (å¿…é¡»ä¿®å¤)

**ä¼˜åŒ–1: Cache ground actions**
- **å®æ–½éš¾åº¦**: æä½ï¼ˆ5åˆ†é’Ÿï¼‰
- **æ€§èƒ½æå‡**: 20-30%æ€»ä½“é€Ÿåº¦
- **é£é™©**: æä½
- **å»ºè®®**: ç«‹å³å®æ–½

### Priority 2: ğŸŸ  Important (å¼ºçƒˆå»ºè®®)

**ä¼˜åŒ–2: Cache goal exploration results**
- **å®æ–½éš¾åº¦**: ä½ï¼ˆ30åˆ†é’Ÿï¼‰
- **æ€§èƒ½æå‡**: å–å†³äºDFAé‡å¤åº¦ï¼ˆ0-90%ï¼‰
- **é£é™©**: ä½ï¼ˆéœ€è¦æ­£ç¡®çš„goal serializationï¼‰
- **å»ºè®®**: å°½å¿«å®æ–½

### Priority 3: ğŸŸ¡ Moderate (å»ºè®®ä¼˜åŒ–)

**ä¼˜åŒ–3: é‡æ„code generationé¿å…é‡å¤**
- **å®æ–½éš¾åº¦**: ä¸­ç­‰ï¼ˆ1-2å°æ—¶ï¼‰
- **æ€§èƒ½æå‡**: 30-50% code generationæ—¶é—´
- **é£é™©**: ä¸­ç­‰ï¼ˆéœ€è¦ä¿®æ”¹AgentSpeakæ–‡ä»¶ç»“æ„ï¼‰
- **å»ºè®®**: åœ¨å®ŒæˆPriority 1-2åå®æ–½

### Priority 4: ğŸŸ¢ Nice-to-have (å¯é€‰)

**ä¼˜åŒ–4: Symmetry reduction**
- **å®æ–½éš¾åº¦**: æé«˜ï¼ˆæ•°å‘¨ï¼‰
- **æ€§èƒ½æå‡**: ç†è®º50%ï¼ˆå®é™…å¯èƒ½æ›´ä½ï¼‰
- **é£é™©**: é«˜ï¼ˆå®¹æ˜“å¼•å…¥bugsï¼‰
- **å»ºè®®**: æš‚ä¸å®æ–½ï¼Œé™¤éæœ‰æ˜ç¡®éœ€æ±‚

---

## å®æ–½è®¡åˆ’

### Phase 1: Quick Wins (1å°æ—¶)
1. âœ… Cache ground actions in ForwardStatePlanner
2. âœ… Add performance metrics logging

### Phase 2: Medium Impact (2-3å°æ—¶)
3. âœ… Implement goal exploration caching
4. âœ… Add cache hit/miss statistics

### Phase 3: Code Quality (2-3å°æ—¶)
5. âœ… Refactor AgentSpeak code generation
6. âœ… Update tests to verify new structure
7. âœ… Update documentation

### Phase 4: Future Work (æš‚ä¸å®æ–½)
- Symmetry reduction (research project level)
- Heuristic search (A* with delete relaxation)
- Partial-order reduction

---

## é¢„æœŸæ€»ä½“æå‡

**å½“å‰æ€§èƒ½** (2 blocks, 2 transitions with same goal):
- Grounding: 34,976æ¬¡è®¡ç®—
- Exploration: 2æ¬¡ Ã— 1093 states = 2186 states
- Code generation: 2æ¬¡å®Œæ•´ç”Ÿæˆ

**ä¼˜åŒ–åæ€§èƒ½** (å®æ–½Phase 1-3):
- Grounding: 32æ¬¡è®¡ç®— (å‡å°‘99.9%)
- Exploration: 1æ¬¡ Ã— 1093 states (å‡å°‘50%)
- Code generation: 1æ¬¡å…±äº«éƒ¨åˆ† + 2æ¬¡goal-specificéƒ¨åˆ† (å‡å°‘40%)

**æ€»ä½“é¢„æœŸæå‡**:
- **50-70% æ€»ä½“é€Ÿåº¦æå‡**
- **30-40% å†…å­˜ä½¿ç”¨å‡å°‘**
- **æ›´æ¸…æ™°çš„ä»£ç ç»“æ„**

---

## æµ‹è¯•è®¡åˆ’

å¯¹äºæ¯ä¸ªä¼˜åŒ–ï¼Œéœ€è¦éªŒè¯ï¼š
1. âœ… åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆç”Ÿæˆçš„plansç›¸åŒï¼‰
2. âœ… æ€§èƒ½æå‡ï¼ˆæµ‹é‡å®é™…æ—¶é—´ï¼‰
3. âœ… è¾¹ç•Œæƒ…å†µï¼ˆempty DFA, single transition, etc.ï¼‰
4. âœ… å‘åå…¼å®¹ï¼ˆç°æœ‰testsä»ç„¶é€šè¿‡ï¼‰
